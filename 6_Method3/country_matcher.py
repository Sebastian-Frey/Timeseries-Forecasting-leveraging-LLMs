{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUCUtQTwjgmyVLGq+PWkEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sebastian-Frey/Timeseries-Forecasting-leveraging-LLMs/blob/main/6_Method3/country_matcher.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62CSVSq1mHD-"
      },
      "outputs": [],
      "source": [
        "# --- Location extractor module for Chronos integration ---\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "import geonamescache\n",
        "import pycountry\n",
        "import pycountry_convert as pc\n",
        "from unidecode import unidecode\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# NLP setup\n",
        "# ----------------------------\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Geo data\n",
        "gc = geonamescache.GeonamesCache()\n",
        "country_names, country_by_name, country_alpha2 = set(), {}, {}\n",
        "for c in pycountry.countries:\n",
        "    canon = c.name\n",
        "    country_alpha2[canon] = getattr(c, \"alpha_2\", None)\n",
        "    for nm in {c.name, getattr(c, \"official_name\", c.name)}:\n",
        "        low = nm.lower()\n",
        "        country_names.add(low)\n",
        "        country_by_name[low] = canon\n",
        "for alias, canon in [\n",
        "    (\"usa\", \"United States\"), (\"u.s.a.\", \"United States\"),\n",
        "    (\"uk\", \"United Kingdom\"), (\"u.k.\", \"United Kingdom\"),\n",
        "    (\"viet nam\", \"Viet Nam\"),\n",
        "]:\n",
        "    country_names.add(alias)\n",
        "    country_by_name[alias] = canon\n",
        "\n",
        "continent_names = {\n",
        "    \"europe\", \"asia\", \"africa\", \"north america\", \"south america\", \"oceania\", \"antarctica\"\n",
        "}\n",
        "continent_by_name = {n: n.title().replace(\" \", \"\") for n in continent_names}\n",
        "continent_by_name[\"north america\"] = \"North America\"\n",
        "continent_by_name[\"south america\"] = \"South America\"\n",
        "\n",
        "def country_to_continent(country_name: str) -> str:\n",
        "    if not country_name or not pc:\n",
        "        return \"Global\"\n",
        "    try:\n",
        "        alpha2 = country_alpha2.get(country_name)\n",
        "        if not alpha2: return \"Global\"\n",
        "        cont_code = pc.country_alpha2_to_continent_code(alpha2)\n",
        "        return pc.convert_continent_code_to_continent_name(cont_code)\n",
        "    except Exception:\n",
        "        return \"Global\"\n",
        "\n",
        "# Cities\n",
        "cities_dict = gc.get_cities()\n",
        "city_names, city_to_country, _city_max_pop = set(), {}, {}\n",
        "for c in cities_dict.values():\n",
        "    nm, pop = c[\"name\"], c.get(\"population\", 0)\n",
        "    city_names.add(nm.lower())\n",
        "    if pop > _city_max_pop.get(nm, -1):\n",
        "        cc = c.get(\"countrycode\")\n",
        "        if cc:\n",
        "            try:\n",
        "                pc_country = pycountry.countries.get(alpha_2=cc)\n",
        "                if pc_country:\n",
        "                    city_to_country[nm] = pc_country.name\n",
        "                    _city_max_pop[nm] = pop\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "CITY_ALIASES = {\n",
        "    \"nyc\": \"New York\", \"la\": \"Los Angeles\", \"vegas\": \"Las Vegas\",\n",
        "    \"dc\": \"Washington\", \"d.c.\": \"Washington\",\n",
        "    \"lisboa\": \"Lisbon\", \"roma\": \"Rome\"\n",
        "}\n",
        "us_states = {\n",
        "    \"alabama\": \"AL\", \"alaska\": \"AK\", \"arizona\": \"AZ\", \"arkansas\": \"AR\",\n",
        "    \"california\": \"CA\", \"colorado\": \"CO\", \"connecticut\": \"CT\", \"delaware\": \"DE\",\n",
        "    \"florida\": \"FL\", \"georgia\": \"GA\", \"hawaii\": \"HI\", \"idaho\": \"ID\",\n",
        "    \"illinois\": \"IL\", \"indiana\": \"IN\", \"iowa\": \"IA\", \"kansas\": \"KS\",\n",
        "    \"kentucky\": \"KY\", \"louisiana\": \"LA\", \"maine\": \"ME\", \"maryland\": \"MD\",\n",
        "    \"massachusetts\": \"MA\", \"michigan\": \"MI\", \"minnesota\": \"MN\", \"mississippi\": \"MS\",\n",
        "    \"missouri\": \"MO\", \"montana\": \"MT\", \"nebraska\": \"NE\", \"nevada\": \"NV\",\n",
        "    \"new hampshire\": \"NH\", \"new jersey\": \"NJ\", \"new mexico\": \"NM\", \"new york\": \"NY\",\n",
        "    \"north carolina\": \"NC\", \"north dakota\": \"ND\", \"ohio\": \"OH\", \"oklahoma\": \"OK\",\n",
        "    \"oregon\": \"OR\", \"pennsylvania\": \"PA\", \"rhode island\": \"RI\", \"south carolina\": \"SC\",\n",
        "    \"south dakota\": \"SD\", \"tennessee\": \"TN\", \"texas\": \"TX\", \"utah\": \"UT\",\n",
        "    \"vermont\": \"VT\", \"virginia\": \"VA\", \"washington\": \"WA\", \"west virginia\": \"WV\",\n",
        "    \"wisconsin\": \"WI\", \"wyoming\": \"WY\", \"district of columbia\": \"DC\"\n",
        "}\n",
        "state_by_name = set(us_states.keys())\n",
        "state_by_code = {v.lower(): k for k, v in us_states.items()}\n",
        "\n",
        "# ----------------------------\n",
        "# Phrase matchers\n",
        "# ----------------------------\n",
        "city_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "country_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "state_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "continent_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "def _nlp_list(terms): return [nlp.make_doc(t) for t in terms]\n",
        "city_matcher.add(\"CITY\", _nlp_list(city_names | set(CITY_ALIASES.keys())))\n",
        "country_matcher.add(\"COUNTRY\", _nlp_list(country_names))\n",
        "state_matcher.add(\"STATE\", _nlp_list(state_by_name | set(state_by_code.keys())))\n",
        "continent_matcher.add(\"CONTINENT\", _nlp_list(continent_names))\n",
        "\n",
        "# ----------------------------\n",
        "# Helper utilities\n",
        "# ----------------------------\n",
        "ABBREV_MAP = {\"ft\": \"fort\", \"st\": \"saint\", \"st.\": \"saint\"}\n",
        "BRAND_BLACKLIST = {\n",
        "    \"adidas\", \"hertz\", \"dollar\", \"avis\", \"enterprise\", \"budget\", \"alamo\", \"sixt\",\n",
        "    \"costco\", \"target\", \"nike\", \"zara\", \"ikea\", \"sprint\", \"verizon\", \"aaa\",\n",
        "    \"mall\", \"outlet\", \"airport\", \"car\", \"rental\", \"rentals\", \"cars\", \"international\",\n",
        "    \"downtown\", \"central\", \"center\", \"north\", \"south\", \"east\", \"west\",\n",
        "}\n",
        "AIRPORT_CITY_REGEX = re.compile(r\"(?<!\\w)([a-zA-Z'.-]+(?:\\s+[a-zA-Z'.-]+)?)\\s+airport\\b\", re.I)\n",
        "\n",
        "def fold_accents(s): return unidecode(s) if (isinstance(s, str) and unidecode) else s\n",
        "def normalize_text(s):\n",
        "    s = fold_accents(s or \"\")\n",
        "    s = s.replace(\"+\", \" \")\n",
        "    s = re.sub(r\"[^a-zA-Z0-9\\s,.'-]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    toks = [ABBREV_MAP.get(tok.lower().strip(\".,\"), tok) for tok in s.split()]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "def resolve_city_alias(name): return CITY_ALIASES.get(name.lower(), name)\n",
        "def city_to_country_lookup(city_name):\n",
        "    for name_variant in (city_name, city_name.title()):\n",
        "        if name_variant in city_to_country:\n",
        "            return city_to_country[name_variant]\n",
        "    return None\n",
        "\n",
        "# ----------------------------\n",
        "# Core extractor\n",
        "# ----------------------------\n",
        "def extract_place_struct(text: str) -> dict:\n",
        "    text_norm = normalize_text(text)\n",
        "    if not text_norm:\n",
        "        return {\"detected_city\": None, \"detected_country\": \"Global\", \"detected_continent\": \"Global\"}\n",
        "\n",
        "    doc = nlp(text_norm)\n",
        "    cities, states, countries, continents, seen = [], [], [], [], set()\n",
        "\n",
        "    # Airport pattern\n",
        "    m = AIRPORT_CITY_REGEX.search(text_norm)\n",
        "    if m:\n",
        "        candidate = m.group(1).strip(\".,' \").lower()\n",
        "        if not any(tok in BRAND_BLACKLIST for tok in candidate.split()):\n",
        "            if candidate in city_names or candidate in CITY_ALIASES:\n",
        "                cities.append(resolve_city_alias(candidate).title())\n",
        "                seen.add(candidate)\n",
        "\n",
        "    # Gazetteer matching\n",
        "    for matcher, sink in [\n",
        "        (city_matcher, cities), (state_matcher, states),\n",
        "        (country_matcher, countries), (continent_matcher, continents)\n",
        "    ]:\n",
        "        for _, s_idx, e_idx in matcher(doc):\n",
        "            span = doc[s_idx:e_idx].text.strip(\",. \").lower()\n",
        "            if span in BRAND_BLACKLIST or span in seen:\n",
        "                continue\n",
        "            if matcher is city_matcher:\n",
        "                sink.append(resolve_city_alias(span).title())\n",
        "            elif matcher is country_matcher:\n",
        "                sink.append(country_by_name.get(span, span.title()))\n",
        "            elif matcher is state_matcher:\n",
        "                sink.append(state_by_code.get(span, span).title())\n",
        "            elif matcher is continent_matcher:\n",
        "                sink.append(continent_by_name.get(span))\n",
        "            seen.add(span)\n",
        "\n",
        "    # Infer country\n",
        "    detected_city = cities[0] if cities else None\n",
        "    detected_country = countries[0] if countries else None\n",
        "    if detected_city and not detected_country:\n",
        "        cc = city_to_country_lookup(detected_city)\n",
        "        if cc: detected_country = cc\n",
        "    if not detected_country and states: detected_country = \"United States\"\n",
        "    detected_country = detected_country or \"Global\"\n",
        "    detected_continent = country_to_continent(detected_country)\n",
        "    if detected_country == \"Global\" and continents:\n",
        "        detected_continent = continents[0]\n",
        "    return {\n",
        "        \"detected_city\": detected_city,\n",
        "        \"detected_country\": detected_country,\n",
        "        \"detected_continent\": detected_continent,\n",
        "    }\n",
        "\n",
        "# --- Add geographical columns ---\n",
        "from typing import Optional, Callable, List\n",
        "\n",
        "\n",
        "def add_location_columns(\n",
        "    df: pd.DataFrame,\n",
        "    keywords: List[str],\n",
        "    progress_callback: Optional[Callable[[int, int], None]] = None,\n",
        "    batch_size: int = 1,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds detected_city / detected_country / detected_continent to df\n",
        "    but only for the specified keywords.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame containing a `keyword` column.\n",
        "    keywords : List[str]\n",
        "        List of keywords to enrich (case-insensitive match against df['keyword']).\n",
        "    progress_callback : callable(current: int, total: int) or None\n",
        "        Optional callback called periodically with (current, total). Safe to pass\n",
        "        a UI-updating function here (it will be wrapped in try/except).\n",
        "    batch_size : int\n",
        "        How often to call the progress callback (every `batch_size` keywords).\n",
        "    \"\"\"\n",
        "    target = df[df[\"keyword\"].str.lower().isin([k.lower() for k in keywords])].copy()\n",
        "    if target.empty:\n",
        "        print(\"⚠️ No matching keywords found in DataFrame.\")\n",
        "        return df\n",
        "\n",
        "    # Work on unique keywords to avoid redundant processing\n",
        "    unique_kws = target[\"keyword\"].drop_duplicates().tolist()\n",
        "    total = len(unique_kws)\n",
        "\n",
        "    if progress_callback is None:\n",
        "        # Fast vectorized path\n",
        "        target[\"_kw_norm\"] = target[\"keyword\"].astype(str).map(normalize_text)\n",
        "        res = target[\"_kw_norm\"].apply(extract_place_struct)\n",
        "        results_df = pd.DataFrame(res.tolist())\n",
        "        mapping_df = pd.concat([target[[\"keyword\"]], results_df], axis=1).drop_duplicates(\"keyword\")\n",
        "    else:\n",
        "        # Iterative path with progress updates\n",
        "        rows = []\n",
        "        for idx, kw in enumerate(unique_kws, start=1):\n",
        "            kw_norm = normalize_text(kw)\n",
        "            struct = extract_place_struct(kw_norm)\n",
        "            rows.append({\"keyword\": kw, **struct})\n",
        "            if (idx % max(1, batch_size) == 0) or (idx == total):\n",
        "                try:\n",
        "                    progress_callback(idx, total)\n",
        "                except Exception:\n",
        "                    # swallow callback exceptions\n",
        "                    pass\n",
        "        mapping_df = pd.DataFrame(rows).drop_duplicates(\"keyword\")\n",
        "\n",
        "    df = df.merge(mapping_df, on=\"keyword\", how=\"left\", validate=\"many_to_one\")\n",
        "    return df"
      ]
    }
  ]
}